---
---

@unpublished{chen2025conrft,
  abbr={Under Review},
  title={ConRFT: A Reinforced Fine-tuning Method for VLA Models via Consistency Policy},
  author={Chen, Yuhui and Tian, Shuai and Zhou, Yingting and Liu, Shugao and Li, Haoran, and Zhao, Dongbin},
  preview={conRFT.png},
  year={2025},
  month={Feb},
  abstract={Vision-Language-Action (VLA) models have shown substantial potential in real-world robotic manipulation. However, fine-tuning these models through supervised learning struggles to achieve robust performance due to limited, inconsistent demonstrations, especially in contact-rich environments. In this paper, we propose a reinforced fine-tuning approach for VLA models, named ConRFT, which consists of offline and online fine-tuning with a unified consistency-based training objective, to address these challenges. In the offline stage, our method integrates behavior cloning and Q-learning to effectively extract policy from a small set of demonstrations and stabilize value estimating. In the online stage, the VLA model is further fine-tuned via consistency policy, with human interventions to ensure safe exploration and high sample efficiency. We evaluate our approach on eight diverse real-world manipulation tasks. It achieves an average success rate of 96.3 % within 45–90 minutes of online fine-tuning, outperforming prior supervised methods with a 144 % improvement in success rate and 1.9x shorter episode length. This work highlights the potential of integrating reinforcement learning to enhance the performance of VLA models for real-world robotic applications.},
  arxiv={2502.05450},
  website={https://cccedric.github.io/conrft/},
  google_scholar_id={UebtZRa9Y70C},
  selected={true},
}

@unpublished{cui2025cl3r,
  abbr={Under Review},
  title={CL3R: 3D Reconstruction and Contrast Learning for Enhanced Robotic Manipulation Representations},
  author={Cui, Wenbo and Zhao, Chengyang and Chen, Yuhui and Li, Haoran and Zhang, Zhizheng and Zhao, Dongbin and Wang, He},
  preview={CL3R.png},
  year={2025},
  month={Feb},
  abstract={Building a robust perception module is crucial for visuomotor policy learning. While recent approaches incorporate pre-trained 2D foundation models into robotic perception modules to leverage their strong semantic understanding, they struggle to capture 3D spatial information and generalize across diverse camera viewpoints. These limitations hinder the policy's effectiveness, especially in fine-grained robotic manipulation scenarios. To address these challenges, we propose CL3R, a novel 3D pre-training framework designed to enhance robotic manipulation policies. Our method integrates both spatial awareness and semantic understanding by employing a point cloud Masked Autoencoder to learn rich 3D representations while leveraging pre-trained 2D foundation models through contrastive learning for efficient semantic knowledge transfer. Additionally, we mitigate camera view ambiguity by unifying the coordinate system across datasets and introduce a point cloud damping method to improve generalizability, enabling robust perception from novel camera viewpoints at test time. Extensive experiments in both simulation and the real world demonstrate the superiority of our method, highlighting its effectiveness in visuomotor policy learning for robotic manipulation.},
  selected={true},
}

@unpublished{chen2024tevir,
  abbr={Under Review},
  title={TeViR: Text-to-Video Reward with Diffusion Models for Efficient Reinforcement Learning},
  author={Chen, Yuhui and Li, Haoran and Jiang, Zhennan and Wen, Haowei, and Zhao, Dongbin},
  preview={tevir.png},
  year={2024},
  month={Jul},
  abstract={Developing scalable and generalizable reward engineering for reinforcement learning (RL) is crucial for creating general-purpose agents, especially in the challenging domain of robotic manipulation. While recent advances in reward engineering with Vision-Language Models (VLMs) have shown promise, their sparse reward nature significantly limits sample efficiency. This paper introduces TeViR, a novel method that leverages a pre-trained text-to-video diffusion model to generate dense rewards by comparing the predicted image sequence with current observations. Experimental results across 11 complex robotic tasks demonstrate that TeViR outperforms traditional methods leveraging sparse rewards and other state-of-the-art (SOTA) methods, achieving better sample efficiency and performance without ground truth environmental rewards. TeViR’s ability to efficiently guide agents in complex environments highlights its potential to advance reinforcement learning applications in robotic manipulation.},
  selected={true},
}

@inproceedings{li2024generalizing,
  abbr={NeurIPS 2024 Poster},
  title={Generalizing Consistency Policy to Visual {RL} with Prioritized Proximal Experience Regularization},
  author={Li, Haoran and Jiang, Zhennan and Chen, Yuhui and Zhao, Dongbin},
  preview={cp3er.png},
  booktitle={The Thirty-eighth Annual Conference on Neural Information Processing Systems, {NIPS}},
  year={2024},
  month={Sep},
  abstract={With high-dimensional state spaces, visual reinforcement learning (RL) faces significant challenges in exploitation and exploration, resulting in low sample efficiency and training stability. As a time-efficient diffusion model, although consistency models have been validated in online state-based RL, it is still an open question whether it can be extended to visual RL. In this paper, we investigate the impact of non-stationary distribution and the actor-critic framework on consistency policy in online RL, and find that consistency policy was unstable during the training, especially in visual RL with the high-dimensional state space. To this end, we suggest sample-based entropy regularization to stabilize the policy training, and propose a consistency policy with prioritized proximal experience regularization (CP3ER) to improve sample efficiency. CP3ER achieves new state-of-the-art (SOTA) performance in 21 tasks across DeepMind control suite and Meta-world. To our knowledge, CP3ER is the first method to apply diffusion/consistency models to visual RL and demonstrates the potential of consistency models in visual RL.},
  arxiv={2410.00051},
  code={https://github.com/jzndd/CP3ER},
  website={https://jzndd.github.io/CP3ER-Page/},
  google_scholar_id={u-x6o8ySG0sC},
  selected={true},
}

@inproceedings{chen2023boosting,
  abbr={AAMAS 2024 Oral},
  title={Boosting Continuous Control with Consistency Policy},
  author={Chen, Yuhui and Li, Haoran and Zhao, Dongbin},
  preview={cpql.png},
  booktitle={Proceedings of the 23rd International Conference on Autonomous Agents and Multiagent Systems, {AAMAS}},
  abstract={Due to its training stability and strong expression, the diffusion model has attracted considerable attention in offline reinforcement learning. However, several challenges have also come with it: 1) The demand for a large number of diffusion steps makes the diffusion-model-based methods time inefficient and limits their applications in real-time control; 2) How to achieve policy improvement with accurate guidance for diffusion model-based policy is still an open problem. Inspired by the consistency model, we propose a novel time-efficiency method named Consistency Policy with Q-Learning (CPQL), which derives action from noise by a single step. By establishing a mapping from the reverse diffusion trajectories to the desired policy, we simultaneously address the issues of time efficiency and inaccurate guidance when updating diffusion model-based policy with the learned Q-function. We demonstrate that CPQL can achieve policy improvement with accurate guidance for offline reinforcement learning, and can be seamlessly extended for online RL tasks. Experimental results indicate that CPQL achieves new state-of-the-art performance on 11 offline and 21 online tasks, significantly improving inference speed by nearly 45 times compared to Diffusion-QL.},
  year={2024},
  month={May},
  code={https://github.com/cccedric/cpql},
  arxiv={2310.06343},
  google_scholar_id={u5HHmVD_uO8C},
  selected={true},
}


