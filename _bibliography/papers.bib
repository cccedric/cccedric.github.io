---
---

@inproceedings{chen2023boosting,
  abbr={AAMAS},
  title={Boosting Continuous Control with Consistency Policy},
  author={Yuhui, Chen and Haoran, Li and Dongbin, Zhao},
  preview={cpql.png},
  booktitle={Proceedings of the 23rd International Conference on Autonomous Agents and Multiagent Systems, {AAMAS} 2024},
  abstract={Due to its training stability and strong expression, the diffusion model has attracted considerable attention in offline reinforcement learning. However, several challenges have also come with it: 1) The demand for a large number of diffusion steps makes the diffusion- model-based methods time inefficient and limits their applications in real-time control; 2) How to achieve policy improvement with accurate guidance for diffusion model-based policy is still an open problem. Inspired by the consistency model, we propose a novel time-efficiency method named Consistency Policy with Q-Learning (CPQL), which derives action from noise by a single step. By estab- lishing a mapping from the reverse diffusion trajectories to the de- sired policy, we simultaneously address the issues of time efficiency and inaccurate guidance when updating diffusion model-based pol- icy with the learned Q-function. We demonstrate that CPQL can achieve policy improvement with accurate guidance for offline re- inforcement learning, and can be seamlessly extended for online RL tasks. Experimental results indicate that CPQL achieves new state-of-the-art performance on 11 offline and 21 online tasks, signif- icantly improving inference speed by nearly 45 times compared to Diffusion-QL.},
  year={2024},
  month={May},
  code={https://github.com/cccedric/cpql},
  arxiv={https://arxiv.org/abs/2310.06343},
  google_scholar_id={u5HHmVD_uO8C},
  selected={true},
}

@inproceedings{li2024generalizing,
  abbr={NIPS},
  title={Generalizing Consistency Policy to Visual {RL} with Prioritized Proximal Experience Regularization},
  author={Haoran, Li and Zhaennan, Jiang and Yuhui, Chen and Dongbin, Zhao},
  preview={cp3er.png},
  booktitle={The Thirty-eighth Annual Conference on Neural Information Processing Systems},
  year={2024},
  month={Sep},
  abstract={With high-dimensional state spaces, visual reinforcement learning (RL) faces significant challenges in exploitation and exploration, resulting in low sample efficiency and training stability. As a time-efficient diffusion model, although consistency models have been validated in online state-based RL, it is still an open question whether it can be extended to visual RL. In this paper, we investigate the impact of non-stationary distribution and the actor-critic framework on consistency policy in online RL, and find that consistency policy was unstable during the training, especially in visual RL with the high-dimensional state space. To this end, we suggest sample-based entropy regularization to stabilize the policy training, and propose a consistency policy with prioritized proximal experience regularization (CP3ER) to improve sample efficiency. CP3ER achieves new state-of-the-art (SOTA) performance in 21 tasks across DeepMind control suite and Meta-world. To our knowledge, CP3ER is the first method to apply diffusion/consistency models to visual RL and demonstrates the potential of consistency models in visual RL.},
  url={https://openreview.net/forum?id=MOFwt8OeXr},
  selected={true},
}